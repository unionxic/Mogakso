# 전체 흐름과 프로젝트 목적
- **RWF-2000이라는 폭력 감지 데이터셋을 활용해, 딥러닝 모델이 주어진 영상에서 폭력 장면을 판단하는 시스템 제작.**

## 접근방식
1. 비디오 데이터를 받아 일정한 개수의 프레임 추출.
2. 추출한 프레임들을 전처리(정규화, 리사이즈)하여 모델에 입력 가능하게.
3. U-Net-like 구조의 MobileNetV2 인코더로부터 공간 특징을 추출.
4. LSTM을 통해 시간적 특징을 학습한 뒤 최종적으로 폭력 여부 판단.

> **Colab 환경에서 진행**됐으며, `RWF-2000/train` 안의 `Fight`, `NonFight` 디렉토리에 데이터가 들어있다.

---

## 데이터 전처리 방식
1. **비디오 파일을 `cv2.VideoCapture`로 열기**
2. 일정한 간격(step)으로 추출. (30fps)
3. 프레임 수가 부족할 경우 마지막 프레임을 반복하여 채움. -> 이 과정을 **패딩(padding)**이라고 함.
4. 사이즈를 `(224, 224)`로 리사이즈.
5. RGB 형태로 통일.
6. NumPy 배열로 변환하여, `(N, 30, 224, 224, 3)` 모양으로 데이터셋 형성. 이때 `N`은 전체 비디오 수.
   - **프레임을 추출하는 이유**: 비디오를 직접 다루기엔 용량이 너무 크고, 모델 입력 형식이 한정적이기에 일정 구간마다 프레임을 뽑아서 고정된 시퀀스로 변환해야 한다.

### 데이터 정규화의 필요성
- **정규화(Normalization)**: 입력값을 일정 범위로 스케일링.
  - MobileNetV2는 `[-1,1]`을 선호.
  - 모델 학습 시, 입력 범위가 너무 크거나 제각각이면 학습 안정성이 떨어지고, 경사 하강 과정에서 불필요한 변동이 커짐.
  - `tf.keras.applications.mobilenet_v2.preprocess_input`을 사용, 각 픽셀값을 적절히 조정하여 모델이 예측하기 좋은 범위로 스케일링.

---

## 모델 구조
### 1. U-Net-like
- U-Net은 원래 이미지 분할(Segmentation)하기 위한 구조, 인코더와 디코더를 대칭적으로 배치해 중간에 스킵 커넥션을 붙여주는 형태.
- 여기서는 U-Net을 전체적으로 사용하지 않고, **인코더 부분을 MobileNetV2 사용. 디코더 부분은 LSTM**으로 시계열 정보를 처리.

### 2. MobileNetV2
- 경량화된 CNN 아키텍처.
  - **특징**:
    - 파라미터 수가 적고, 연산량이 적어서 **임베디드/모바일 환경**에 적합.
    - 정확도 꽤 좋음.
  - **역할**:
    - 각 프레임에서 **공간적(Spatial) 특징**을 추출.
    - 즉, 프레임에 담긴 객체, 모션, 배경 등을 CNN으로 인코딩해 **1280차원 특징 벡터**로 압축.

### 3. LSTM(Long Short-Term Memory)
- RNN의 한 종류, 시계열 정보를 잘 처리하도록 고안됨.
  - **특징**:
    - 장기 의존성(Long-term Dependency)을 학습할 수 있음.
  - **역할**:
    - MobileNetV2가 뽑은 30개의 특징 벡터를 순차적으로 입력받아, **폭력 여부를 결정하기 위한 시간적 패턴 파악**.

### 4. 왜 CNN + LSTM 조합일까?
- 각각 공간 정보, 시간 정보 분석에 뛰어나다.
- 그래서 비디오는 2D 이미지가 시간축에 따라 변하는 3D 데이터이므로, **공간 + 시간 두 가지 축을 모두 다뤄야** 저렇게 사용한 것.

---

## 활성화 함수(Activation Function)
1. **ReLU**:
   - 구현이 간단하고, 경사 소실 문제가 적다.
   - 뉴런이 음수가 되면 0으로 죽이고, 양수에서는 선형이므로 학습이 빠르다.
   - 은닉층(DNN의 중간 Dense 레이어나 CNN 레이어 등)에 주로 사용.

2. **Sigmoid**:
   - 출력이 (0,1) 사이로 매핑되어 확률 해석에 유리.
   - 다만, 큰 음수나 양수에서 기울기가 거의 0이라서 경사 소실 우려가 있음.

---

## 모델 학습 과정
1. **손실 함수**: `binary_crossentropy`
   - 이진 분류에서 사용. 예측값과 실제값 차이를 로그 형태로 계산.

2. **옵티마이저(optimizer)**: `Adam`
   - Adaptive Moment Estimation.
   - 학습률을 적절히 조절하면서 경사를 갱신해, 빠른 학습과 적절한 수렴을 만듦.

3. **학습률 스케줄**: `ReduceLROnPlateau`
   - 일정 에폭 이상 개선이 없으면 학습률을 줄여 세밀하게 학습하도록 유도.

4. **정확도와 손실 곡선 해석**:
   - 학습 정확도가 급상승 -> 모델이 훈련 데이터에 맞춰서 잘 적응 중.
   - 검증 정확도(Val Accuracy)가 어떤 지점에서 정체 또는 하락한다면 -> **오버피팅 가능성.**
   - 학습 손실(Train Loss)와 검증 손실(Val Loss)가 모두 감소하는지 확인.
     - 검증 손실이 어느 순간부터 감소하지 않고 정체되거나 올라가면 학습률 조정 또는 정규화 방법 검토.

---

## 최종 모델 평가
- 1열: TN, FP / 2열: FN, TP로 구성.

| **구분**       | **설명**                                     |
|----------------|---------------------------------------------|
| **TN**         | 실제 비폭력을 비폭력으로 잘 예측.             |
| **FP**         | 실제 비폭력인데, 폭력으로 잘못 예측.          |
| **FN**         | 실제 폭력인데 비폭력으로 잘못 예측.           |
| **TP**         | 실제 폭력인데 폭력으로 잘 예측.              |
| **Precision**  | 모델이 폭력이라고 예측한 것 중 진짜 폭력이 몇 %인가. |
| **F1-Score**   | 정밀도와 재현율의 조화평균 -> 종합적으로 성능이 어느 정도인지 파악하기 좋음. |

### Confusion Matrix 예시
![Confusion Matrix](https://github.com/user-attachments/assets/64eb4c55-912f-4452-8d2b-7fcce73f5d97)

---

## 로스율과 학습률 분석
1. **로스율**:
   - 학습이 진행됨에 따라 **트레이닝 손실(train loss)이 감소**하는지.
   - **검증 손실(val loss)이 정체**되거나 다시 커지지 않는지 확인해야 함.

2. **학습률**:
   - 초기에는 `1e-4`처럼 적당히 큰 값을 써서 빠르게 수렴.
   - 오버피팅이 일어나거나 더 세밀한 학습이 필요할 때는 `ReduceLROnPlateau`에 의해 `1e-5`, `1e-6`처럼 낮아짐.

3. **에폭(Epoch)**:
   - 한 번의 에폭이란 전체 데이터를 한 바퀴 학습하는 것.
   - 에폭이 늘어날수록 정확도가 올라가지만, Val Accuracy가 정체되면 일반화 성능이 한계라는 것.

---

### 인코더(Encoder)와 디코더(Decoder)는 뭐하는 걸까
1.  Encoder
    * 원래 데이터를 압축해서 중요한 정보를 뽑아낸다.
    * 예를 들어, 비디오 프레임을 받아서 그 안의 의미있는 특징을 추출, 이후 벡터 형태로 압축
    * 이 프로젝트에서는 MobileNetV2를 인코더로 사용해 각 비디오 프레임에서 1280차원 벡터로 압축된 정보를 추출.
2. Decoder
    * 인코더가 추출한 정보를 다시 원래 형태로 복원하거나 결과를 출력.
    * 예를 들어, 텍스트 생성이나 이미지 복원에서 디코더가 자주 사용됨.
    * 이 프로젝트에서는 디코더 대신 LSTM을 사용해 시간적 특징을 학습.
* 이 두가지 구조를 사용하는 이유는, 중요한 특징만 학습하거나, 압축된 정보를 기반으로 더 나은 예측을 수행할 수 있기 때문에 사용.

### MobileNetV2 인코더에서 1280차원 특징 벡터로 압축하는 이유??
1. 효율적인 학습
    * 원래 비디오 프레임은 (224x224x3) 크기의 이미지로, 총 150528개의 숫자로 표현.
    * 이를 1280차원 벡터로 압축하면 모델이 학습해야 할 파라미터가 줄어들어 메모리 사용량과 학습 속도를 줄일 수 있다.
2. 중요한 특징만 유지
    * MobileNetV2는 사전 학습된 CNN으로, 이미 이미지의 가장 중요한 정보를 추출하도록 훈련되어있다.
    * 예를 들어 특정 패턴, 색상, 모양 등 분유에 필요한 모든 정보를 1280차원에 압축해 전달.
3. 프로젝트의 필요성    
    * 비디오 데이터를 다룰 때, 모든 프레임의 픽셀 값을 처리하면 연산량이 매우 많아져 비효율적
    * MobilNetV2는 효율성을 유지하면서도 높은 정확도를 제공한는 경량화 모델.

### 경사 소실(Vanishing Gradient)란 무엇이고, 왜 일어나면 안되는지.
1. 경사 소실이란?
    * 역전파 과정에서, 기울기가 계속 작아져서 0에 가까워지는 현상.
    * 주로 활성화 함수와 네트워크 층이 너무 깊을 때 발생.
2. 왜 문제일까?
    * 기울기가 0에 가까워지면, 네트워크의 초기 층들이 학습을 멈춤.
    * 이는 모델이 깊어질수록 성능 향상을 기대하기 어렵게 만듦.
3. 어떻게 해결할까?
    * RELU 사용: 기울기가 0으로 죽는 영역을 피함.
    * Batch Normalization: 입력 데이터의 분포를 정규화하여 학습 안정성 향상.
    * Residual Connection(ResNet): 네트워크의 일부 출력을 뒤쪽으로 바로 연결해 경사 소실 방지.

### 은닉층(DNN, Dense Layer, CNN 등) 계층 설명
1. DNN(Deep Neural Network)
    * 다층 신경망은 입력층(Input Layer), 출력층(Output Layer), 은닉층(Hidden Layer)로 구성.
    * 은닉층: 입력과 출력이 연결하면서 데이터를 변환하는 계층.
2. Dense Layer (완전 연결층)
    * 모든 뉴런이 이전 층의 모든 뉴런과 연결된 구조.
    * 주로 마지막 출력층 직전에 사용되며, 최종 분류나 회귀를 수행.
3. CNN
    * 이미지를 다루기 위해 고안된 모델.
    * 입력 이미지에서 특징을 추출하기 위해 합성곱 연산을 수행.
4. RNN
    * 순차 데이터(시계열, 텍스트)를 처리하기 위해 고안된 모델.
    * 이전 출력값을 다음 입력으로 사용.
### 오버피팅이란 뭘까?
1. 오버피팅이란.
    * 모델이 학습 데이터에는 너무 잘 맞지만, 새로운 데이터에는 성능이 떨어지는 현상.
    * 학습 데이터의 잡음까지 외워버린 상태.
2. 징후
    * Train Loss는 계속 줄어들지만, Val Loss가 일정 시점에서 증가.
    * Train Accuracy는 높지만, Val Accurary는 정체 또는 하락.
3. 해결방법
    * Dropout: 일부 뉴런을 무작위로 끄고 학습.
    * Early Stopping: 검증 손실이 더 이상 개선되지 않으면 학습 중단.
    * 데이터 증강: 데이터의 다양성을 인위적으로 늘림.
### 학습률을 조정하면 뭐가 달라지나?
1. 학습률이란?
    * 기울기 업데이트 시 가중치 변경 정도를 결정하는 값.
2. 학습률이 너무 크면?
    * 최적 값 주변을 건너뛰고, 수렴하지 않음.
3. 학습률이 너무 작으면?
    * 학습이 너무 느려지고, 지역 최적값에 갇힐 위험.
4. 학습률 조정 효과
    * 초반에는 빠르게 큰 변화(lr=1e-4)
    * 후반에는 느린 세밀한 조정(lr=1e-5)을 통해 최적화




















